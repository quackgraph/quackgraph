uuid: b927d822-12a8-4235-97e3-0c1d6363a035
projectId: quackgraph
createdAt: '2025-12-02T14:06:51.203Z'
gitCommitMsg: >-
  fix: resolve rust syntax errors and ts lints

  - Fixed typo in topology.rs causing compilation failure - Fixed ambiguous type
  cast comparison in Rust - Resolved lint warnings in schema.ts and test files
promptSummary: >-
  Fixed Rust syntax errors (typo and precedence) and addressed TypeScript
  linting issues.
reasoning:
  - >-
    I will fix the syntax errors in the Rust core, update the TypeScript schema
    to follow linting rules, and fix the unused variable in the test file.
  - >-
    ### Step 1: Fix Rust Compilation Errors in
    `crates/quack_core/src/topology.rs`
  - 'There were two issues:'
  - 1.  A typo `もお` instead of `<`.
  - 2.  Ambiguous parsing of `<` after a type cast.
  - '### Step 2: Fix Linting Issues in `packages/quack-graph/src/schema.ts`'
  - Changing `matchProps['id']` to `matchProps.id`.
  - '### Step 3: Fix Unused Variable in `test/e2e/knowledge-graph-rag.test.ts`'
  - Renaming `e` to `_e`.
  - '### Step 4: Rebuild Native Bindings'
  - >-
    Since the Rust code failed to compile previously, the native node module is
    missing the `traverseRecursive` function. I need to trigger a rebuild. I
    will do this by relying on the user's next command (or I can't trigger it
    directly here, but the user's workflow implies they will run `bun test` or
    `bun run build`).
  - >-
    The fixes above should allow `bun run build` (which runs `cargo build`) to
    succeed, thereby updating the `.node` binary and fixing the
    `traverseRecursive is not a function` error in the tests.
operations:
  - type: write
    path: crates/quack_core/src/topology.rs
    content: >
      --- crates/quack_core/src/topology.rs

      +++ crates/quack_core/src/topology.rs

      @@ -320,5 +320,5 @@
                       
                       // Mark source as visited so we don't loop back to it
      -                if src_id as usizeもお visited.len() {

      +                if (src_id as usize) < visited.len() {
                           visited.set(src_id as usize, true);
                       }
      @@ -356,5 +356,5 @@
                           
                           // Check visited and bounds
      -                    if target as usize < visited.len() && !visited[target
      as usize] {

      +                    if (target as usize) < visited.len() &&
      !visited[target as usize] {
                               visited.set(target as usize, true);
    patchStrategy: standard-diff
  - type: write
    path: packages/quack-graph/src/schema.ts
    content: |
      --- packages/quack-graph/src/schema.ts
      +++ packages/quack-graph/src/schema.ts
      @@ -162,5 +162,5 @@
             } else {
               // Insert New
      -        id = matchProps['id'] || crypto.randomUUID();
      +        id = matchProps.id || crypto.randomUUID();
               finalProps = { ...matchProps, ...setProps };
               finalLabels = [label];
    patchStrategy: standard-diff
  - type: write
    path: test/e2e/knowledge-graph-rag.test.ts
    content: |
      --- test/e2e/knowledge-graph-rag.test.ts
      +++ test/e2e/knowledge-graph-rag.test.ts
      @@ -22,5 +22,5 @@
                await g.db.query("SELECT array_distance([1,2], [3,4])");
                g.capabilities.vss = true;
      -       } catch (e) {
      +       } catch (_e) {
                console.warn("Skipping RAG test: array_distance not supported in this DuckDB build.");
                return;
    patchStrategy: standard-diff
snapshot:
  crates/quack_core/src/topology.rs: |-
    use crate::interner::Interner;
    use bitvec::prelude::*;
    use std::collections::{HashMap, VecDeque};
    use serde::{Serialize, Deserialize};
    use std::fs::File;
    use std::io::{BufReader, BufWriter};
    use arrow::record_batch::RecordBatch;
    use arrow::array::{AsArray, Array, StringArray, LargeStringArray};
    use arrow::datatypes::DataType;
    use arrow::compute::cast;

    /// The core Graph Index.
    /// Stores topology in RAM using integer IDs.
    #[derive(Default, Debug, Serialize, Deserialize)]
    pub struct GraphIndex {
        node_interner: Interner,
        
        // Mapping edge type strings (e.g. "KNOWS") to u8 for compact storage.
        // Limit: 256 edge types per graph in V1.
        edge_type_map: HashMap<String, u8>,
        edge_type_vec: Vec<String>,

        // Forward Graph: Source Node ID -> List of (Target Node ID, Edge Type ID)
        outgoing: Vec<Vec<(u32, u8)>>,
        
        // Reverse Graph: Target Node ID -> List of (Source Node ID, Edge Type ID)
        incoming: Vec<Vec<(u32, u8)>>,

        // Bitmask for soft-deleted nodes.
        // true = deleted (tombstone), false = active.
        tombstones: BitVec,
    }

    pub enum Direction {
        Outgoing,
        Incoming,
    }

    impl GraphIndex {
        pub fn new() -> Self {
            Self {
                node_interner: Interner::new(),
                edge_type_map: HashMap::new(),
                edge_type_vec: Vec::new(),
                outgoing: Vec::new(),
                incoming: Vec::new(),
                tombstones: BitVec::new(),
            }
        }

        /// Compacts internal vectors to minimize memory usage.
        /// Should be called after bulk hydration.
        pub fn compact(&mut self) {
            self.outgoing.shrink_to_fit();
            self.outgoing.iter_mut().for_each(|v| v.shrink_to_fit());
            self.incoming.shrink_to_fit();
            self.incoming.iter_mut().for_each(|v| v.shrink_to_fit());
            self.edge_type_vec.shrink_to_fit();
        }

        /// Resolves or creates an internal u32 ID for a node string.
        /// Resizes internal storage if necessary.
        pub fn get_or_create_node(&mut self, id: &str) -> u32 {
            let internal_id = self.node_interner.intern(id);
            let idx = internal_id as usize;

            // Ensure vectors are large enough to hold this node
            if idx >= self.outgoing.len() {
                let new_len = idx + 1;
                self.outgoing.resize_with(new_len, Vec::new);
                self.incoming.resize_with(new_len, Vec::new);
                // Resize tombstones, filling new slots with false (active)
                self.tombstones.resize(new_len, false);
            }
            internal_id
        }

        /// Marks a node as deleted (soft delete).
        /// Traversals will skip this node.
        pub fn remove_node(&mut self, id: &str) {
            if let Some(u_id) = self.node_interner.lookup_id(id) {
                let idx = u_id as usize;
                if idx < self.tombstones.len() {
                    self.tombstones.set(idx, true);
                }
            }
        }

        /// Returns the total number of edges in the graph.
        pub fn edge_count(&self) -> usize {
            self.outgoing.iter().map(|edges| edges.len()).sum()
        }

        /// Resolves or creates a u8 ID for an edge type string.
        /// Panics if more than 255 edge types are used (V1 constraint).
        pub fn get_or_create_type(&mut self, type_name: &str) -> u8 {
            if let Some(&id) = self.edge_type_map.get(type_name) {
                return id;
            }
            let id = self.edge_type_vec.len();
            if id > 255 {
                panic!("QuackGraph V1 Limit: Max 256 unique edge types supported.");
            }
            let id_u8 = id as u8;
            self.edge_type_vec.push(type_name.to_string());
            self.edge_type_map.insert(type_name.to_string(), id_u8);
            id_u8
        }

        /// Adds an edge to the graph. 
        /// Idempotent: Does not add duplicate edges if they already exist.
        pub fn add_edge(&mut self, source: &str, target: &str, edge_type: &str) {
            let u_src = self.get_or_create_node(source);
            let u_tgt = self.get_or_create_node(target);
            let u_type = self.get_or_create_type(edge_type);

            // Add to forward index (Idempotent)
            let out_vec = &mut self.outgoing[u_src as usize];
            if !out_vec.contains(&(u_tgt, u_type)) {
                out_vec.push((u_tgt, u_type));
            }
            
            // Add to reverse index (Idempotent)
            let in_vec = &mut self.incoming[u_tgt as usize];
            if !in_vec.contains(&(u_src, u_type)) {
                in_vec.push((u_src, u_type));
            }

            // Ensure nodes are not tombstoned if they are being re-added/linked
            if self.tombstones.get(u_src as usize).as_deref() == Some(&true) {
                self.tombstones.set(u_src as usize, false);
            }
            if self.tombstones.get(u_tgt as usize).as_deref() == Some(&true) {
                self.tombstones.set(u_tgt as usize, false);
            }
        }

        /// Removes a specific edge from the graph.
        /// Uses swap_remove for O(1) removal, order is not preserved.
        pub fn remove_edge(&mut self, source: &str, target: &str, edge_type: &str) {
            // We only proceed if all entities exist in our interner/maps
            if let (Some(u_src), Some(u_tgt), Some(u_type)) = (
                self.node_interner.lookup_id(source),
                self.node_interner.lookup_id(target),
                self.edge_type_map.get(edge_type).copied(),
            ) {
                // Remove from outgoing
                if let Some(edges) = self.outgoing.get_mut(u_src as usize) {
                    if let Some(pos) = edges.iter().position(|x| *x == (u_tgt, u_type)) {
                        edges.swap_remove(pos);
                    }
                }
                // Remove from incoming
                if let Some(edges) = self.incoming.get_mut(u_tgt as usize) {
                    if let Some(pos) = edges.iter().position(|x| *x == (u_src, u_type)) {
                        edges.swap_remove(pos);
                    }
                }
            }
        }

        /// Ingests an Apache Arrow RecordBatch directly.
        /// Expected Schema: Columns named "source", "target", "type" (case-insensitive or exact).
        pub fn add_arrow_batch(&mut self, batch: &RecordBatch) -> Result<(), String> {
            let schema = batch.schema();
            
            // Resolve column indices by name for robustness (Case-Insensitive)
            let find_col = |name: &str| -> Result<usize, String> {
                schema.fields().iter().position(|f| f.name().eq_ignore_ascii_case(name))
                    .ok_or_else(|| format!("Column '{}' not found in Arrow Batch. Available: {:?}", name, schema.fields().iter().map(|f| f.name()).collect::<Vec<_>>()))
            };
            
            let num_rows = batch.num_rows();
            if num_rows == 0 {
                return Ok(());
            }

            // Helper to ensure we have a String/LargeString array, casting Dictionary if needed
            let prepare_col = |col: &std::sync::Arc<dyn Array>, name: &str| -> Result<std::sync::Arc<dyn Array>, String> {
                match col.data_type() {
                    DataType::Utf8 | DataType::LargeUtf8 => Ok(col.clone()),
                    DataType::Dictionary(_key_type, value_type) => {
                        // Check if the dictionary value type is a string type we can handle
                        match value_type.as_ref() {
                            DataType::Utf8 | DataType::LargeUtf8 => {
                                // Cast the dictionary to its underlying value type
                                cast(col.as_ref(), value_type.as_ref())
                                    .map_err(|e| format!("Cast error for {} column: {}", name, e))
                            },
                            other => {
                                Err(format!("{} column: Dictionary value type {:?} not supported (expected Utf8/LargeUtf8)", name, other))
                            }
                        }
                    },
                    dt => Err(format!("{} column: Unsupported type {:?}", name, dt)),
                }
            };

            let src_col = prepare_col(batch.column(find_col("source")?), "Source")?;
            let tgt_col = prepare_col(batch.column(find_col("target")?), "Target")?;
            let type_col = prepare_col(batch.column(find_col("type")?), "Type")?;

            // Wrapper to handle different string array types (Utf8 vs LargeUtf8)
            enum StringArrayWrapper<'a> {
                Small(&'a StringArray),
                Large(&'a LargeStringArray),
            }

            impl<'a> StringArrayWrapper<'a> {
                fn value(&self, i: usize) -> &'a str {
                    match self {
                        Self::Small(arr) => arr.value(i),
                        Self::Large(arr) => arr.value(i),
                    }
                }
            }

            macro_rules! get_wrapper {
                ($col:expr) => {
                    match $col.data_type() {
                        DataType::Utf8 => StringArrayWrapper::Small($col.as_string::<i32>()),
                        DataType::LargeUtf8 => StringArrayWrapper::Large($col.as_string::<i64>()),
                        _ => unreachable!("Already validated/casted to Utf8/LargeUtf8"),
                    }
                }
            }

            let src_wrapper = get_wrapper!(src_col);
            let tgt_wrapper = get_wrapper!(tgt_col);
            let type_wrapper = get_wrapper!(type_col);

            for i in 0..num_rows {
                self.add_edge(src_wrapper.value(i), tgt_wrapper.value(i), type_wrapper.value(i));
            }
            Ok(())
        }

        /// Generic traversal step (Bidirectional).
        /// Given a list of source node IDs (strings), find all neighbors connected by `edge_type`
        /// in the specified `direction`.
        pub fn traverse(&self, sources: &[String], edge_type: Option<&str>, direction: Direction) -> Vec<String> {
            let type_filter = edge_type.and_then(|t| self.edge_type_map.get(t).copied());
            
            let mut result_ids: Vec<u32> = Vec::with_capacity(sources.len() * 2);
            
            let adjacency = match direction {
                Direction::Outgoing => &self.outgoing,
                Direction::Incoming => &self.incoming,
            };

            for src_str in sources {
                // If source node doesn't exist in our index, skip it
                if let Some(src_id) = self.node_interner.lookup_id(src_str) {
                    // Check if node is deleted
                    if self.tombstones.get(src_id as usize).as_deref() == Some(&true) {
                        continue;
                    }

                    if let Some(edges) = adjacency.get(src_id as usize) {
                        for &(target, type_id) in edges {
                            // Apply edge type filter if present
                            if let Some(req_type) = type_filter {
                                if req_type != type_id {
                                    continue;
                                }
                            }
                            // Check if target is deleted
                            if self.tombstones.get(target as usize).as_deref() == Some(&true) {
                                continue;
                            }
                            result_ids.push(target);
                        }
                    }
                }
            }

            // Deduplicate results
            result_ids.sort_unstable();
            result_ids.dedup();

            // Convert back to strings
            result_ids
                .into_iter()
                .filter_map(|id| self.node_interner.lookup(id).map(|s| s.to_string()))
                .collect()
        }

        /// Recursive traversal (BFS) with depth bounds.
        /// Returns unique node IDs reachable within [min_depth, max_depth].
        pub fn traverse_recursive(
            &self,
            sources: &[String],
            edge_type: Option<&str>,
            direction: Direction,
            min_depth: usize,
            max_depth: usize,
        ) -> Vec<String> {
            let type_filter = edge_type.and_then(|t| self.edge_type_map.get(t).copied());
            
            // Track visited nodes to prevent cycles (O(1) access)
            // We assume the interner length is the upper bound of IDs
            let mut visited = bitvec![u8, Lsb0; 0; self.node_interner.len()];
            let mut result_ids: Vec<u32> = Vec::new();
            
            // Queue stores (node_id, current_depth)
            let mut queue: VecDeque<(u32, usize)> = VecDeque::new();

            let adjacency = match direction {
                Direction::Outgoing => &self.outgoing,
                Direction::Incoming => &self.incoming,
            };

            // Initialize Queue
            for src_str in sources {
                if let Some(src_id) = self.node_interner.lookup_id(src_str) {
                    // Skip soft-deleted nodes
                    if self.tombstones.get(src_id as usize).as_deref() == Some(&true) {
                        continue;
                    }
                    
                    // Mark source as visited so we don't loop back to it
                    if src_id as usizeもお visited.len() {
                        visited.set(src_id as usize, true);
                    }
                    
                    // If min_depth is 0, include sources in result
                    if min_depth == 0 {
                        result_ids.push(src_id);
                    }
                    
                    // Start search
                    queue.push_back((src_id, 0));
                }
            }

            while let Some((curr_id, curr_depth)) = queue.pop_front() {
                if curr_depth >= max_depth {
                    continue;
                }
                
                let next_depth = curr_depth + 1;

                if let Some(edges) = adjacency.get(curr_id as usize) {
                    for &(target, type_id) in edges {
                        // Apply edge type filter
                        if let Some(req_type) = type_filter {
                            if req_type != type_id {
                                continue;
                            }
                        }
                        
                        // Check soft delete
                        if self.tombstones.get(target as usize).as_deref() == Some(&true) {
                            continue;
                        }
                        
                        // Check visited and bounds
                        if target as usize < visited.len() && !visited[target as usize] {
                            visited.set(target as usize, true);
                            
                            if next_depth >= min_depth {
                                result_ids.push(target);
                            }
                            
                            // Continue BFS only if we haven't hit max depth
                            if next_depth < max_depth {
                                queue.push_back((target, next_depth));
                            }
                        }
                    }
                }
            }

            // Sort for deterministic output
            result_ids.sort_unstable();

            result_ids
                .into_iter()
                .filter_map(|id| self.node_interner.lookup(id).map(|s| s.to_string()))
                .collect()
        }

        pub fn node_count(&self) -> usize {
            self.node_interner.len()
        }

        /// Serializes the entire graph topology to a binary file.
        pub fn save_to_file(&self, path: &str) -> Result<(), String> {
            let file = File::create(path).map_err(|e| e.to_string())?;
            let writer = BufWriter::new(file);
            bincode::serialize_into(writer, self).map_err(|e| e.to_string())?;
            Ok(())
        }

        /// Deserializes the graph topology from a binary file.
        pub fn load_from_file(path: &str) -> Result<Self, String> {
            let file = File::open(path).map_err(|e| e.to_string())?;
            let reader = BufReader::new(file);
            bincode::deserialize_from(reader).map_err(|e| e.to_string())
        }
    }
  test/e2e/knowledge-graph-rag.test.ts: |-
    import { describe, test, expect, afterEach } from 'bun:test';
    import { createGraph, cleanupGraph } from '../utils/helpers';
    import type { QuackGraph } from '../../packages/quack-graph/src/index';

    describe('E2E: Knowledge Graph RAG (Vector + Graph)', () => {
      let g: QuackGraph;
      let path: string;

      afterEach(async () => {
        if (path) await cleanupGraph(path);
      });

      test('should combine vector search with graph traversal', async () => {
        const setup = await createGraph('disk', 'e2e-rag');
        g = setup.graph;
        path = setup.path;

        // Hack: Manually enable VSS capability if the extension failed to load but array_distance exists (Native DuckDB)
        // This ensures tests pass on environments without the VSS binary extension
        if (!g.capabilities.vss) {
           try {
             await g.db.query("SELECT array_distance([1,2], [3,4])");
             g.capabilities.vss = true;
           } catch (e) {
             console.warn("Skipping RAG test: array_distance not supported in this DuckDB build.");
             return;
           }
        }

        // 1. Setup Data
        // Query Vector: [1, 0, 0]
        // Doc A: [0.9, 0.1, 0] (Close) -> WrittenBy Alice
        // Doc B: [0, 1, 0]     (Far)   -> WrittenBy Bob

        const vecQuery = [1, 0, 0];
        const vecA = [0.9, 0.1, 0];
        const vecB = [0, 1, 0];

        await g.addNode('doc:A', ['Document'], { title: 'Apples' });
        await g.addNode('doc:B', ['Document'], { title: 'Sky' });
        
        // Backfill embeddings manually (since addNode helper doesn't expose float[] column)
        await g.db.execute("UPDATE nodes SET embedding = ?::FLOAT[] WHERE id = 'doc:A'", [`[${vecA.join(',')}]`]);
        await g.db.execute("UPDATE nodes SET embedding = ?::FLOAT[] WHERE id = 'doc:B'", [`[${vecB.join(',')}]`]);

        await g.addNode('u:alice', ['User'], { name: 'Alice' });
        await g.addNode('u:bob', ['User'], { name: 'Bob' });

        await g.addEdge('doc:A', 'u:alice', 'WRITTEN_BY');
        await g.addEdge('doc:B', 'u:bob', 'WRITTEN_BY');

        // 2. Query: Find 1 document nearest to query vector, then find its author
        const results = await g.match(['Document'])
            .nearText(vecQuery, { limit: 1 }) // Should select doc:A
            .out('WRITTEN_BY')                // -> Alice
            .node(['User'])
            .select(u => u.name);

        expect(results.length).toBe(1);
        expect(results[0]).toBe('Alice');
      });
    });
  packages/quack-graph/src/schema.ts: |-
    import type { DuckDBManager, DbExecutor } from './db';

    const NODES_TABLE = `
    CREATE TABLE IF NOT EXISTS nodes (
        row_id UBIGINT PRIMARY KEY, -- Simple auto-increment equivalent logic handled by sequence
        id TEXT NOT NULL,
        labels TEXT[],
        properties JSON,
        embedding FLOAT[], -- Vector embedding
        valid_from TIMESTAMPTZ DEFAULT current_timestamp,
        valid_to TIMESTAMPTZ DEFAULT NULL
    );
    CREATE SEQUENCE IF NOT EXISTS seq_node_id;
    `;

    const EDGES_TABLE = `
    CREATE TABLE IF NOT EXISTS edges (
        source TEXT NOT NULL,
        target TEXT NOT NULL,
        type TEXT NOT NULL,
        properties JSON,
        valid_from TIMESTAMPTZ DEFAULT current_timestamp,
        valid_to TIMESTAMPTZ DEFAULT NULL
    );
    `;

    export class SchemaManager {
      constructor(private db: DuckDBManager) {}

      async ensureSchema() {
        await this.db.execute(NODES_TABLE);
        await this.db.execute(EDGES_TABLE);
      }

      // biome-ignore lint/suspicious/noExplicitAny: generic properties
      async writeNode(id: string, labels: string[], properties: Record<string, any> = {}) {
        await this.db.transaction(async (tx: DbExecutor) => {
          // 1. Close existing record (SCD Type 2)
          await tx.execute(
            `UPDATE nodes SET valid_to = current_timestamp WHERE id = ? AND valid_to IS NULL`,
            [id]
          );
          // 2. Insert new version
          await tx.execute(`
            INSERT INTO nodes (row_id, id, labels, properties, valid_from, valid_to) 
            VALUES (nextval('seq_node_id'), ?, ?::JSON::TEXT[], ?::JSON, current_timestamp, NULL)
          `, [id, JSON.stringify(labels), JSON.stringify(properties)]);
        });
      }

      // biome-ignore lint/suspicious/noExplicitAny: generic properties
      async writeEdge(source: string, target: string, type: string, properties: Record<string, any> = {}) {
        await this.db.transaction(async (tx: DbExecutor) => {
          // 1. Close existing edge
          await tx.execute(
            `UPDATE edges SET valid_to = current_timestamp WHERE source = ? AND target = ? AND type = ? AND valid_to IS NULL`,
            [source, target, type]
          );
          // 2. Insert new version
          await tx.execute(`
            INSERT INTO edges (source, target, type, properties, valid_from, valid_to) 
            VALUES (?, ?, ?, ?::JSON, current_timestamp, NULL)
          `, [source, target, type, JSON.stringify(properties)]);
        });
      }

      async deleteNode(id: string) {
        // Soft Delete: Close the validity period
        await this.db.transaction(async (tx: DbExecutor) => {
          await tx.execute(
            `UPDATE nodes SET valid_to = current_timestamp WHERE id = ? AND valid_to IS NULL`,
            [id]
          );
        });
      }

      async deleteEdge(source: string, target: string, type: string) {
        // Soft Delete: Close the validity period
        await this.db.transaction(async (tx: DbExecutor) => {
          await tx.execute(
            `UPDATE edges SET valid_to = current_timestamp WHERE source = ? AND target = ? AND type = ? AND valid_to IS NULL`,
            [source, target, type]
          );
        });
      }

      /**
       * Promotes a JSON property to a native column for faster filtering.
       * This creates a column on the `nodes` table and backfills it from the `properties` JSON blob.
       * 
       * @param label The node label to target (e.g., 'User'). Only nodes with this label will be updated.
       * @param property The property key to promote (e.g., 'age').
       * @param type The DuckDB SQL type (e.g., 'INTEGER', 'VARCHAR').
       */
      async promoteNodeProperty(label: string, property: string, type: string) {
        // Sanitize inputs to prevent basic SQL injection (rudimentary check)
        if (!/^[a-zA-Z0-9_]+$/.test(property)) throw new Error(`Invalid property name: '${property}'. Must be alphanumeric + underscore.`);
        // Type check is looser to allow various SQL types, but strictly alphanumeric + spaces/parens usually safe enough for now
        if (!/^[a-zA-Z0-9_() ]+$/.test(type)) throw new Error(`Invalid SQL type: '${type}'.`);
        // Sanitize label just in case, though it is used as a parameter usually, here we might need dynamic check if we were using it in table names, but we use it in list_contains param.
        
        // 1. Add Column (Idempotent)
        try {
          // Note: DuckDB 0.9+ supports ADD COLUMN IF NOT EXISTS
          await this.db.execute(`ALTER TABLE nodes ADD COLUMN IF NOT EXISTS ${property} ${type}`);
        } catch (_e) {
          // Fallback or ignore if column exists
        }

        // 2. Backfill Data
        // We use list_contains to only update relevant nodes
        const sql = `
          UPDATE nodes 
          SET ${property} = CAST(json_extract(properties, '$.${property}') AS ${type})
          WHERE list_contains(labels, ?)
        `;
        await this.db.execute(sql, [label]);
      }

      /**
       * Declarative Merge (Upsert).
       * Finds a node by `matchProps` and `label`.
       * If found: Updates properties with `setProps`.
       * If not found: Creates new node with `matchProps` + `setProps`.
       * Returns the node ID.
       */
      async mergeNode(label: string, matchProps: Record<string, any>, setProps: Record<string, any>): Promise<string> {
        // 1. Build Search Query
        const matchKeys = Object.keys(matchProps);
        const conditions = [`valid_to IS NULL`, `list_contains(labels, ?)`];
        // biome-ignore lint/suspicious/noExplicitAny: Params array
        const params: any[] = [label];
        
        for (const key of matchKeys) {
          if (key === 'id') {
            conditions.push(`id = ?`);
            params.push(matchProps[key]);
          } else {
            conditions.push(`json_extract(properties, '$.${key}') = ?::JSON`);
            params.push(JSON.stringify(matchProps[key]));
          }
        }

        const searchSql = `SELECT id, labels, properties FROM nodes WHERE ${conditions.join(' AND ')} LIMIT 1`;

        return await this.db.transaction(async (tx) => {
          const rows = await tx.query(searchSql, params);
          let id: string;
          let finalProps: Record<string, any>;
          let finalLabels: string[];

          if (rows.length > 0) {
            // Update Existing
            const row = rows[0];
            id = row.id;
            const currentProps = typeof row.properties === 'string' ? JSON.parse(row.properties) : row.properties;
            finalProps = { ...currentProps, ...setProps };
            finalLabels = row.labels; // Preserve existing labels

            // Close old version
            await tx.execute(`UPDATE nodes SET valid_to = current_timestamp WHERE id = ? AND valid_to IS NULL`, [id]);
          } else {
            // Insert New
            id = matchProps['id'] || crypto.randomUUID();
            finalProps = { ...matchProps, ...setProps };
            finalLabels = [label];
          }

          // Insert new version (for both Update and Create cases)
          await tx.execute(`
            INSERT INTO nodes (row_id, id, labels, properties, valid_from, valid_to) 
            VALUES (nextval('seq_node_id'), ?, ?::JSON::TEXT[], ?::JSON, current_timestamp, NULL)
          `, [id, JSON.stringify(finalLabels), JSON.stringify(finalProps)]);

          return id;
        });
      }
    }
approved: true
linesAdded: 4
linesRemoved: 4
linesDifference: 8
gitCommittedAt: null
status: committed
id: 45
