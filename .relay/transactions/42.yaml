uuid: b1d9f8c4-e8f0-4a8b-8a1a-8c8f0e8f0a8b
projectId: quackgraph
createdAt: '2025-12-02T13:58:54.705Z'
gitCommitMsg: >-
  feat(core): implement recursive BFS traversal

  Adds efficient, depth-bounded breadth-first search to the Rust graph engine.
  Exposes `traverseRecursive` in the native bindings to support variable-length
  path queries (e.g. `*1..5`). Uses a BitVec for O(1) visited checks to handle
  cycles.
promptSummary: >-
  Implemented recursive BFS traversal logic in the Rust core and exposed it via
  NAPI. This adds `traverse_recursive` to the graph index, supporting
  `min_depth` and `max_depth` parameters with cycle detection via a BitVec
  visited set.
reasoning:
  - >-
    Step 1: Implementing the recursive BFS engine in Rust Core
    (`crates/quack_core`) and exposing it via the Native Bridge
    (`packages/native`). This enables variable-length path traversals (e.g.,
    `*1..5`) efficiently in memory.
operations:
  - type: write
    path: crates/quack_core/src/topology.rs
    content: >
      --- crates/quack_core/src/topology.rs

      +++ crates/quack_core/src/topology.rs

      @@ -2,3 +2,3 @@
       use bitvec::prelude::*;
      -use std::collections::HashMap;

      +use std::collections::{HashMap, VecDeque};
       use serde::{Serialize, Deserialize};
      @@ -218,2 +218,65 @@
       
      +    /// Recursive traversal (BFS) with depth bounds.

      +    /// Returns unique node IDs reachable within [min_depth, max_depth].

      +    pub fn traverse_recursive(

      +        &self,

      +        sources: &[String],

      +        edge_type: Option<&str>,

      +        direction: Direction,

      +        min_depth: usize,

      +        max_depth: usize,

      +    ) -> Vec<String> {

      +        let type_filter = edge_type.and_then(|t|
      self.edge_type_map.get(t).copied());

      +        

      +        // Track visited nodes to prevent cycles (O(1) access)

      +        // We assume the interner length is the upper bound of IDs

      +        let mut visited = bitvec![u8, Lsb0; 0; self.node_interner.len()];

      +        let mut result_ids: Vec<u32> = Vec::new();

      +        

      +        // Queue stores (node_id, current_depth)

      +        let mut queue: VecDeque<(u32, usize)> = VecDeque::new();

      +

      +        let adjacency = match direction {

      +            Direction::Outgoing => &self.outgoing,

      +            Direction::Incoming => &self.incoming,

      +        };

      +

      +        // Initialize Queue

      +        for src_str in sources {

      +            if let Some(src_id) = self.node_interner.lookup_id(src_str) {

      +                // Skip soft-deleted nodes

      +                if self.tombstones.get(src_id as usize).as_deref() ==
      Some(&true) {

      +                    continue;

      +                }

      +                

      +                // Mark source as visited so we don't loop back to it

      +                if src_id as usizeもお visited.len() {

      +                    visited.set(src_id as usize, true);

      +                }

      +                

      +                // If min_depth is 0, include sources in result

      +                if min_depth == 0 {

      +                    result_ids.push(src_id);

      +                }

      +                

      +                // Start search

      +                queue.push_back((src_id, 0));

      +            }

      +        }

      +

      +        while let Some((curr_id, curr_depth)) = queue.pop_front() {

      +            if curr_depth >= max_depth {

      +                continue;

      +            }

      +            

      +            let next_depth = curr_depth + 1;

      +

      +            if let Some(edges) = adjacency.get(curr_id as usize) {

      +                for &(target, type_id) in edges {

      +                    // Apply edge type filter

      +                    if let Some(req_type) = type_filter {

      +                        if req_type != type_id {

      +                            continue;

      +                        }

      +                    }

      +                    

      +                    // Check soft delete

      +                    if self.tombstones.get(target as usize).as_deref() ==
      Some(&true) {

      +                        continue;

      +                    }

      +                    

      +                    // Check visited and bounds

      +                    if target as usize < visited.len() && !visited[target
      as usize] {

      +                        visited.set(target as usize, true);

      +                        

      +                        if next_depth >= min_depth {

      +                            result_ids.push(target);

      +                        }

      +                        

      +                        // Continue BFS only if we haven't hit max depth

      +                        if next_depth < max_depth {

      +                            queue.push_back((target, next_depth));

      +                        }

      +                    }

      +                }

      +            }

      +        }

      +

      +        // Sort for deterministic output

      +        result_ids.sort_unstable();

      +

      +        result_ids

      +            .into_iter()

      +            .filter_map(|id| self.node_interner.lookup(id).map(|s|
      s.to_string()))

      +            .collect()

      +    }

      +
           pub fn node_count(&self) -> usize {
    patchStrategy: standard-diff
  - type: write
    path: packages/native/src/lib.rs
    content: >
      --- packages/native/src/lib.rs

      +++ packages/native/src/lib.rs

      @@ -74,2 +74,18 @@
       
      +    /// Performs a recursive traversal (BFS) with depth bounds.

      +    /// Returns unique node IDs reachable within [min_depth, max_depth].

      +    #[napi]

      +    pub fn traverse_recursive(&self, sources: Vec<String>, edge_type:
      Option<String>, direction: Option<String>, min_depth: Option<u32>,
      max_depth: Option<u32>) -> Vec<String> {

      +        let dir = match direction.as_deref() {

      +            Some("in") | Some("IN") => Direction::Incoming,

      +            _ => Direction::Outgoing,

      +        };

      +        

      +        let min = min_depth.unwrap_or(1) as usize;

      +        let max = max_depth.unwrap_or(1) as usize;

      +        

      +        self.inner.traverse_recursive(&sources, edge_type.as_deref(),
      dir, min, max)

      +    }

      +
           /// Returns the number of nodes in the interned index.
           /// Useful for debugging hydration.
    patchStrategy: standard-diff
  - type: write
    path: packages/native/index.d.ts
    content: >
      --- packages/native/index.d.ts

      +++ packages/native/index.d.ts

      @@ -23,2 +23,7 @@
         traverse(sources: Array<string>, edgeType?: string | undefined | null, direction?: string | undefined | null): Array<string>
      +  /**

      +   * Performs a recursive traversal (BFS) with depth bounds.

      +   * Returns unique node IDs reachable within [min_depth, max_depth].

      +   */

      +  traverseRecursive(sources: Array<string>, edgeType?: string | undefined
      | null, direction?: string | undefined | null, minDepth?: number |
      undefined | null, maxDepth?: number | undefined | null): Array<string>
         /**
    patchStrategy: standard-diff
snapshot:
  packages/native/src/lib.rs: |-
    #![deny(clippy::all)]

    use napi::bindgen_prelude::*;
    use napi_derive::napi;
    use quack_core::{GraphIndex, Direction};
    use arrow::ipc::reader::StreamReader;
    use std::io::Cursor;

    #[napi]
    pub struct NativeGraph {
        inner: GraphIndex,
    }

    #[napi]
    impl NativeGraph {
        #[napi(constructor)]
        pub fn new() -> Self {
            Self {
                inner: GraphIndex::new(),
            }
        }

        #[napi]
        pub fn add_node(&mut self, id: String) {
            self.inner.get_or_create_node(&id);
        }

        /// Hydrates the graph from an Arrow IPC stream (Buffer).
        /// Zero-copy (mostly) data transfer from DuckDB.
        #[napi]
        pub fn load_arrow_ipc(&mut self, buffer: Buffer) -> napi::Result<()> {
            let cursor = Cursor::new(buffer.as_ref());
            let reader = StreamReader::try_new(cursor, None).map_err(|e| napi::Error::from_reason(e.to_string()))?;

            for batch in reader {
                let batch = batch.map_err(|e| napi::Error::from_reason(e.to_string()))?;
                self.inner.add_arrow_batch(&batch).map_err(napi::Error::from_reason)?;
            }
            Ok(())
        }

        /// Compacts the graph's memory usage.
        /// Call this after hydration to reclaim unused capacity in the adjacency lists.
        #[napi]
        pub fn compact(&mut self) {
            self.inner.compact();
        }

        #[napi]
        pub fn add_edge(&mut self, source: String, target: String, edge_type: String) {
            self.inner.add_edge(&source, &target, &edge_type);
        }

        #[napi]
        pub fn remove_node(&mut self, id: String) {
            self.inner.remove_node(&id);
        }

        #[napi]
        pub fn remove_edge(&mut self, source: String, target: String, edge_type: String) {
            self.inner.remove_edge(&source, &target, &edge_type);
        }

        /// Performs a single-hop traversal (bfs-step).
        /// Returns unique neighbor IDs.
        #[napi]
        pub fn traverse(&self, sources: Vec<String>, edge_type: Option<String>, direction: Option<String>) -> Vec<String> {
            let dir = match direction.as_deref() {
                Some("in") | Some("IN") => Direction::Incoming,
                _ => Direction::Outgoing,
            };
            self.inner.traverse(&sources, edge_type.as_deref(), dir)
        }

        /// Returns the number of nodes in the interned index.
        /// Useful for debugging hydration.
        #[napi(getter)]
        pub fn node_count(&self) -> u32 {
            // We cast to u32 because exposing usize to JS can be finicky depending on napi version,
            // though napi usually handles numbers well. Safe for V1.
            self.inner.node_count() as u32
        }

        #[napi(getter)]
        pub fn edge_count(&self) -> u32 {
            self.inner.edge_count() as u32
        }

        #[napi]
        pub fn save_snapshot(&self, path: String) -> napi::Result<()> {
            self.inner.save_to_file(&path).map_err(napi::Error::from_reason)
        }

        #[napi]
        pub fn load_snapshot(&mut self, path: String) -> napi::Result<()> {
            let loaded = GraphIndex::load_from_file(&path).map_err(napi::Error::from_reason)?;
            self.inner = loaded;
            Ok(())
        }
    }

    impl Default for NativeGraph {
        fn default() -> Self {
            Self::new()
        }
    }
  crates/quack_core/src/topology.rs: |-
    use crate::interner::Interner;
    use bitvec::prelude::*;
    use std::collections::HashMap;
    use serde::{Serialize, Deserialize};
    use std::fs::File;
    use std::io::{BufReader, BufWriter};
    use arrow::record_batch::RecordBatch;
    use arrow::array::{AsArray, Array, StringArray, LargeStringArray};
    use arrow::datatypes::DataType;
    use arrow::compute::cast;

    /// The core Graph Index.
    /// Stores topology in RAM using integer IDs.
    #[derive(Default, Debug, Serialize, Deserialize)]
    pub struct GraphIndex {
        node_interner: Interner,
        
        // Mapping edge type strings (e.g. "KNOWS") to u8 for compact storage.
        // Limit: 256 edge types per graph in V1.
        edge_type_map: HashMap<String, u8>,
        edge_type_vec: Vec<String>,

        // Forward Graph: Source Node ID -> List of (Target Node ID, Edge Type ID)
        outgoing: Vec<Vec<(u32, u8)>>,
        
        // Reverse Graph: Target Node ID -> List of (Source Node ID, Edge Type ID)
        incoming: Vec<Vec<(u32, u8)>>,

        // Bitmask for soft-deleted nodes.
        // true = deleted (tombstone), false = active.
        tombstones: BitVec,
    }

    pub enum Direction {
        Outgoing,
        Incoming,
    }

    impl GraphIndex {
        pub fn new() -> Self {
            Self {
                node_interner: Interner::new(),
                edge_type_map: HashMap::new(),
                edge_type_vec: Vec::new(),
                outgoing: Vec::new(),
                incoming: Vec::new(),
                tombstones: BitVec::new(),
            }
        }

        /// Compacts internal vectors to minimize memory usage.
        /// Should be called after bulk hydration.
        pub fn compact(&mut self) {
            self.outgoing.shrink_to_fit();
            self.outgoing.iter_mut().for_each(|v| v.shrink_to_fit());
            self.incoming.shrink_to_fit();
            self.incoming.iter_mut().for_each(|v| v.shrink_to_fit());
            self.edge_type_vec.shrink_to_fit();
        }

        /// Resolves or creates an internal u32 ID for a node string.
        /// Resizes internal storage if necessary.
        pub fn get_or_create_node(&mut self, id: &str) -> u32 {
            let internal_id = self.node_interner.intern(id);
            let idx = internal_id as usize;

            // Ensure vectors are large enough to hold this node
            if idx >= self.outgoing.len() {
                let new_len = idx + 1;
                self.outgoing.resize_with(new_len, Vec::new);
                self.incoming.resize_with(new_len, Vec::new);
                // Resize tombstones, filling new slots with false (active)
                self.tombstones.resize(new_len, false);
            }
            internal_id
        }

        /// Marks a node as deleted (soft delete).
        /// Traversals will skip this node.
        pub fn remove_node(&mut self, id: &str) {
            if let Some(u_id) = self.node_interner.lookup_id(id) {
                let idx = u_id as usize;
                if idx < self.tombstones.len() {
                    self.tombstones.set(idx, true);
                }
            }
        }

        /// Returns the total number of edges in the graph.
        pub fn edge_count(&self) -> usize {
            self.outgoing.iter().map(|edges| edges.len()).sum()
        }

        /// Resolves or creates a u8 ID for an edge type string.
        /// Panics if more than 255 edge types are used (V1 constraint).
        pub fn get_or_create_type(&mut self, type_name: &str) -> u8 {
            if let Some(&id) = self.edge_type_map.get(type_name) {
                return id;
            }
            let id = self.edge_type_vec.len();
            if id > 255 {
                panic!("QuackGraph V1 Limit: Max 256 unique edge types supported.");
            }
            let id_u8 = id as u8;
            self.edge_type_vec.push(type_name.to_string());
            self.edge_type_map.insert(type_name.to_string(), id_u8);
            id_u8
        }

        /// Adds an edge to the graph. 
        /// Idempotent: Does not add duplicate edges if they already exist.
        pub fn add_edge(&mut self, source: &str, target: &str, edge_type: &str) {
            let u_src = self.get_or_create_node(source);
            let u_tgt = self.get_or_create_node(target);
            let u_type = self.get_or_create_type(edge_type);

            // Add to forward index (Idempotent)
            let out_vec = &mut self.outgoing[u_src as usize];
            if !out_vec.contains(&(u_tgt, u_type)) {
                out_vec.push((u_tgt, u_type));
            }
            
            // Add to reverse index (Idempotent)
            let in_vec = &mut self.incoming[u_tgt as usize];
            if !in_vec.contains(&(u_src, u_type)) {
                in_vec.push((u_src, u_type));
            }

            // Ensure nodes are not tombstoned if they are being re-added/linked
            if self.tombstones.get(u_src as usize).as_deref() == Some(&true) {
                self.tombstones.set(u_src as usize, false);
            }
            if self.tombstones.get(u_tgt as usize).as_deref() == Some(&true) {
                self.tombstones.set(u_tgt as usize, false);
            }
        }

        /// Removes a specific edge from the graph.
        /// Uses swap_remove for O(1) removal, order is not preserved.
        pub fn remove_edge(&mut self, source: &str, target: &str, edge_type: &str) {
            // We only proceed if all entities exist in our interner/maps
            if let (Some(u_src), Some(u_tgt), Some(u_type)) = (
                self.node_interner.lookup_id(source),
                self.node_interner.lookup_id(target),
                self.edge_type_map.get(edge_type).copied(),
            ) {
                // Remove from outgoing
                if let Some(edges) = self.outgoing.get_mut(u_src as usize) {
                    if let Some(pos) = edges.iter().position(|x| *x == (u_tgt, u_type)) {
                        edges.swap_remove(pos);
                    }
                }
                // Remove from incoming
                if let Some(edges) = self.incoming.get_mut(u_tgt as usize) {
                    if let Some(pos) = edges.iter().position(|x| *x == (u_src, u_type)) {
                        edges.swap_remove(pos);
                    }
                }
            }
        }

        /// Ingests an Apache Arrow RecordBatch directly.
        /// Expected Schema: Columns named "source", "target", "type" (case-insensitive or exact).
        pub fn add_arrow_batch(&mut self, batch: &RecordBatch) -> Result<(), String> {
            let schema = batch.schema();
            
            // Resolve column indices by name for robustness (Case-Insensitive)
            let find_col = |name: &str| -> Result<usize, String> {
                schema.fields().iter().position(|f| f.name().eq_ignore_ascii_case(name))
                    .ok_or_else(|| format!("Column '{}' not found in Arrow Batch. Available: {:?}", name, schema.fields().iter().map(|f| f.name()).collect::<Vec<_>>()))
            };
            
            let num_rows = batch.num_rows();
            if num_rows == 0 {
                return Ok(());
            }

            // Helper to ensure we have a String/LargeString array, casting Dictionary if needed
            let prepare_col = |col: &std::sync::Arc<dyn Array>, name: &str| -> Result<std::sync::Arc<dyn Array>, String> {
                match col.data_type() {
                    DataType::Utf8 | DataType::LargeUtf8 => Ok(col.clone()),
                    DataType::Dictionary(_key_type, value_type) => {
                        // Check if the dictionary value type is a string type we can handle
                        match value_type.as_ref() {
                            DataType::Utf8 | DataType::LargeUtf8 => {
                                // Cast the dictionary to its underlying value type
                                cast(col.as_ref(), value_type.as_ref())
                                    .map_err(|e| format!("Cast error for {} column: {}", name, e))
                            },
                            other => {
                                Err(format!("{} column: Dictionary value type {:?} not supported (expected Utf8/LargeUtf8)", name, other))
                            }
                        }
                    },
                    dt => Err(format!("{} column: Unsupported type {:?}", name, dt)),
                }
            };

            let src_col = prepare_col(batch.column(find_col("source")?), "Source")?;
            let tgt_col = prepare_col(batch.column(find_col("target")?), "Target")?;
            let type_col = prepare_col(batch.column(find_col("type")?), "Type")?;

            // Wrapper to handle different string array types (Utf8 vs LargeUtf8)
            enum StringArrayWrapper<'a> {
                Small(&'a StringArray),
                Large(&'a LargeStringArray),
            }

            impl<'a> StringArrayWrapper<'a> {
                fn value(&self, i: usize) -> &'a str {
                    match self {
                        Self::Small(arr) => arr.value(i),
                        Self::Large(arr) => arr.value(i),
                    }
                }
            }

            macro_rules! get_wrapper {
                ($col:expr) => {
                    match $col.data_type() {
                        DataType::Utf8 => StringArrayWrapper::Small($col.as_string::<i32>()),
                        DataType::LargeUtf8 => StringArrayWrapper::Large($col.as_string::<i64>()),
                        _ => unreachable!("Already validated/casted to Utf8/LargeUtf8"),
                    }
                }
            }

            let src_wrapper = get_wrapper!(src_col);
            let tgt_wrapper = get_wrapper!(tgt_col);
            let type_wrapper = get_wrapper!(type_col);

            for i in 0..num_rows {
                self.add_edge(src_wrapper.value(i), tgt_wrapper.value(i), type_wrapper.value(i));
            }
            Ok(())
        }

        /// Generic traversal step (Bidirectional).
        /// Given a list of source node IDs (strings), find all neighbors connected by `edge_type`
        /// in the specified `direction`.
        pub fn traverse(&self, sources: &[String], edge_type: Option<&str>, direction: Direction) -> Vec<String> {
            let type_filter = edge_type.and_then(|t| self.edge_type_map.get(t).copied());
            
            let mut result_ids: Vec<u32> = Vec::with_capacity(sources.len() * 2);
            
            let adjacency = match direction {
                Direction::Outgoing => &self.outgoing,
                Direction::Incoming => &self.incoming,
            };

            for src_str in sources {
                // If source node doesn't exist in our index, skip it
                if let Some(src_id) = self.node_interner.lookup_id(src_str) {
                    // Check if node is deleted
                    if self.tombstones.get(src_id as usize).as_deref() == Some(&true) {
                        continue;
                    }

                    if let Some(edges) = adjacency.get(src_id as usize) {
                        for &(target, type_id) in edges {
                            // Apply edge type filter if present
                            if let Some(req_type) = type_filter {
                                if req_type != type_id {
                                    continue;
                                }
                            }
                            // Check if target is deleted
                            if self.tombstones.get(target as usize).as_deref() == Some(&true) {
                                continue;
                            }
                            result_ids.push(target);
                        }
                    }
                }
            }

            // Deduplicate results
            result_ids.sort_unstable();
            result_ids.dedup();

            // Convert back to strings
            result_ids
                .into_iter()
                .filter_map(|id| self.node_interner.lookup(id).map(|s| s.to_string()))
                .collect()
        }

        pub fn node_count(&self) -> usize {
            self.node_interner.len()
        }

        /// Serializes the entire graph topology to a binary file.
        pub fn save_to_file(&self, path: &str) -> Result<(), String> {
            let file = File::create(path).map_err(|e| e.to_string())?;
            let writer = BufWriter::new(file);
            bincode::serialize_into(writer, self).map_err(|e| e.to_string())?;
            Ok(())
        }

        /// Deserializes the graph topology from a binary file.
        pub fn load_from_file(path: &str) -> Result<Self, String> {
            let file = File::open(path).map_err(|e| e.to_string())?;
            let reader = BufReader::new(file);
            bincode::deserialize_from(reader).map_err(|e| e.to_string())
        }
    }
  packages/native/index.d.ts: |
    /* tslint:disable */
    /* eslint-disable */

    /* auto-generated by NAPI-RS */

    export declare class NativeGraph {
      constructor()
      addNode(id: string): void
      /**
       * Hydrates the graph from an Arrow IPC stream (Buffer).
       * Zero-copy (mostly) data transfer from DuckDB.
       */
      loadArrowIpc(buffer: Buffer): void
      /**
       * Compacts the graph's memory usage.
       * Call this after hydration to reclaim unused capacity in the adjacency lists.
       */
      compact(): void
      addEdge(source: string, target: string, edgeType: string): void
      removeNode(id: string): void
      removeEdge(source: string, target: string, edgeType: string): void
      /**
       * Performs a single-hop traversal (bfs-step).
       * Returns unique neighbor IDs.
       */
      traverse(sources: Array<string>, edgeType?: string | undefined | null, direction?: string | undefined | null): Array<string>
      /**
       * Returns the number of nodes in the interned index.
       * Useful for debugging hydration.
       */
      get nodeCount(): number
      get edgeCount(): number
      saveSnapshot(path: string): void
      loadSnapshot(path: string): void
    }
approved: true
linesAdded: 116
linesRemoved: 1
linesDifference: 117
gitCommittedAt: null
status: committed
id: 42
