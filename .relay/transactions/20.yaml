uuid: 4a2d3e1f-8b9c-4d5a-6b7c-8d9e0f1a2b3c
projectId: quackgraph
createdAt: '2025-12-02T09:59:05.436Z'
gitCommitMsg: >-
  fix(lint): resolve biome and clippy diagnostics

  - Updates biome.json schema and fixes deprecated ignore patterns. - Removes
  unused import in Rust core. - Removes ignored Cargo profile from native
  package. - Suppresses unavoidable `any` types in TS with biome-ignore
  comments. - Fixes unused variables in catch blocks. - Refactors property
  access in `graph.ts` to use a getter.
promptSummary: >-
  Fixed linting errors (Biome, Rust Clippy) including unused variables, any
  types, and configuration issues.
reasoning:
  - >-
    Of course. It seems the last attempt failed due to a patch mismatch. I'll
    regenerate the changes with careful attention to context to ensure they
    apply correctly. Here are the fixes for the linting and compilation errors.
  - '### 1. Fix Rust Unused Imports'
  - >-
    I'll start by fixing the unused import in
    `crates/quack_core/src/topology.rs`.
  - '### 2. Fix Cargo Workspace Profiles'
  - >-
    The `[profile.release]` section in `packages/native/Cargo.toml` is ignored
    and should be removed.
  - '### 3. Fix Biome Configuration'
  - >-
    I will update the Biome schema version and correct the deprecated ignore
    patterns by removing the trailing `/**`.
  - '### 4. Fix TypeScript DB Wrapper (`db.ts`)'
  - >-
    This was the file that failed to patch. I will apply several changes here:
    refactoring the `path` property to avoid bracket notation access, and adding
    linter suppressions for the necessary use of `any` with generic SQL
    parameters and results.
  - '### 5. Fix TypeScript Graph Logic (`graph.ts`)'
  - >-
    I will update the code to use the new `db.path` getter, remove an
    unnecessary `as any` cast for a `Buffer`, and add suppressions for other
    unavoidable `any` types.
  - '### 6. Fix TypeScript Schema Logic (`schema.ts`)'
  - >-
    Here, I'll switch to `import type`, rename an unused catch variable, and add
    suppressions for generic property objects.
  - '### 7. Fix Test Helpers (`helpers.ts`)'
  - Finally, I'll fix the unused variable in the test helper file.
  - '### 8. Final YAML'
operations:
  - type: write
    path: crates/quack_core/src/topology.rs
    content: >+
      --- crates/quack_core/src/topology.rs

      +++ crates/quack_core/src/topology.rs

      @@ -5,7 +5,7 @@
       use std::fs::File;
       use std::io::{BufReader, BufWriter};
       use arrow::record_batch::RecordBatch;
      -use arrow::array::{AsArray, Array, ArrayRef, StringArray,
      LargeStringArray};

      +use arrow::array::{AsArray, Array, StringArray, LargeStringArray};
       use arrow::datatypes::DataType;
       
       /// The core Graph Index.

    patchStrategy: standard-diff
  - type: write
    path: packages/native/Cargo.toml
    content: |+
      --- packages/native/Cargo.toml
      +++ packages/native/Cargo.toml
      @@ -18,6 +18,3 @@
       
       [build-dependencies]
       napi-build = "2.0.1"
      -
      -[profile.release]
      -lto = true

    patchStrategy: standard-diff
  - type: write
    path: biome.json
    content: |+
      --- biome.json
      +++ biome.json
      @@ -1,5 +1,5 @@
       {
      -  "$schema": "https://biomejs.dev/schemas/1.9.4/schema.json",
      +  "$schema": "https://biomejs.dev/schemas/2.3.8/schema.json",
         "vcs": {
           "enabled": true,
           "clientKind": "git",
      @@ -9,10 +9,10 @@
           "ignoreUnknown": true,
           "includes": [
             "**",
      -      "!target/**",
      -      "!dist/**",
      -      "!node_modules/**",
      +      "!target",
      +      "!dist",
      +      "!node_modules",
             "!**/*.node",
             "!repomix.config.json"
           ]

    patchStrategy: standard-diff
  - type: write
    path: packages/quack-graph/src/db.ts
    content: >+
      --- packages/quack-graph/src/db.ts

      +++ packages/quack-graph/src/db.ts

      @@ -2,33 +2,43 @@
       
       // Interface for operations that can be performed within a transaction or globally
       export interface DbExecutor {
      -  execute(sql: string, params?: any[]): Promise<void>;

      -  query(sql: string, params?: any[]): Promise<any[]>;

      +  // biome-ignore lint/suspicious/noExplicitAny: SQL params are generic

      +  execute(sql: string, params?: any[]): Promise<void>;

      +  // biome-ignore lint/suspicious/noExplicitAny: SQL results are generic

      +  query(sql: string, params?: any[]): Promise<any[]>;
       }
       
       export class DuckDBManager implements DbExecutor {
         private db: Database | null = null;
      -  private path: string;

      +  private _path: string;
       
         constructor(path: string = ':memory:') {
      -    this.path = path;

      +    this._path = path;
         }
       
         async init() {
           if (!this.db) {
      -      this.db = await Database.create(this.path);

      +      this.db = await Database.create(this._path);
           }
         }
       
      +  get path(): string {

      +    return this._path;

      +  }

      +
         getDb(): Database {
           if (!this.db) {
             throw new Error('Database not initialized. Call init() first.');
           }
           return this.db;
         }
       
      -  async execute(sql: string, params: any[] = []): Promise<void> {

      +  // biome-ignore lint/suspicious/noExplicitAny: SQL params

      +  async execute(sql: string, params: any[] = []): Promise<void> {
           const db = this.getDb();
           await db.run(sql, params);
         }
       
      -  async query(sql: string, params: any[] = []): Promise<any[]> {

      +  // biome-ignore lint/suspicious/noExplicitAny: SQL results

      +  async query(sql: string, params: any[] = []): Promise<any[]> {
           const db = this.getDb();
           return await db.all(sql, params);
         }
      @@ -42,11 +52,15 @@
           
           // Create a transaction-bound executor wrapper
           const txExecutor: DbExecutor = {
      -      execute: async (sql: string, params: any[] = []) => {

      +      // biome-ignore lint/suspicious/noExplicitAny: SQL params

      +      execute: async (sql: string, params: any[] = []) => {
               await conn.run(sql, params);
             },
      -      query: async (sql: string, params: any[] = []) => {

      +      // biome-ignore lint/suspicious/noExplicitAny: SQL results

      +      query: async (sql: string, params: any[] = []) => {
               return await conn.all(sql, params);
             }
           };
      @@ -74,13 +88,15 @@
          * Executes a query and returns the raw Apache Arrow IPC Buffer.
          * Used for high-speed hydration.
          */
      -  async queryArrow(sql: string, params: any[] = []): Promise<Uint8Array>
      {

      +  // biome-ignore lint/suspicious/noExplicitAny: SQL params

      +  async queryArrow(sql: string, params: any[] = []): Promise<Uint8Array>
      {
           const db = this.getDb();
           
           return new Promise((resolve, reject) => {
             // Hack: Access underlying node-duckdb connection/database
             // duckdb-async instance holds 'db' property which is the native Database
      -      const rawDb = (db as any).db || db;

      +      // biome-ignore lint/suspicious/noExplicitAny: DuckDB internals

      +      const rawDb = (db as any).db || db;
       
             if (!rawDb) return reject(new Error("Could not access underlying DuckDB Native instance."));
       
      @@ -99,7 +115,8 @@
       
             // Try Database.arrowIPCAll (available in newer node-duckdb)
             if (typeof rawDb.arrowIPCAll === 'function') {
      -        rawDb.arrowIPCAll(sql, params, (err: any, result: any) => {

      +        // biome-ignore lint/suspicious/noExplicitAny: internal callback
      signature

      +        rawDb.arrowIPCAll(sql, params, (err: any, result: any) => {
                 if (err) return reject(err);
                 // Result is usually Array<Uint8Array> (batches)
                 if (Array.isArray(result)) {
      @@ -117,7 +134,8 @@
                     : (rawConn && typeof rawConn.arrowIPCAll === 'function' ? rawConn : null);
       
                   if (target) {
      -               target.arrowIPCAll(sql, params, (err: any, result: any) =>
      {

      +               // biome-ignore lint/suspicious/noExplicitAny: internal
      callback signature

      +               target.arrowIPCAll(sql, params, (err: any, result: any) =>
      {
                         if (err) return reject(err);
                         if (Array.isArray(result)) {
                           resolve(mergeBatches(result));

    patchStrategy: standard-diff
  - type: write
    path: packages/quack-graph/src/graph.ts
    content: >+
      --- packages/quack-graph/src/graph.ts

      +++ packages/quack-graph/src/graph.ts

      @@ -84,20 +84,23 @@
                // Napi-rs expects a Buffer or equivalent
                // Buffer.from is zero-copy in Node for Uint8Array usually, or cheap copy
                // We cast to any to satisfy the generated TS definitions which might expect Buffer
      +         // biome-ignore lint/suspicious/noExplicitAny: Buffer cast
      compatibility for NAPI
                const bufferForNapi = Buffer.isBuffer(ipcBuffer) 
                   ? ipcBuffer 
                   : Buffer.from(ipcBuffer);
                   
      -         this.native.loadArrowIpc(bufferForNapi as any);

      +         this.native.loadArrowIpc(bufferForNapi);
       
                // Reclaim memory after burst hydration
                this.native.compact();
             }
      -    } catch (e: any) {

      -      throw new Error(`Hydration Error: ${e.message}`);

      +    // biome-ignore lint/suspicious/noExplicitAny: error handling

      +    } catch (e: any) {

      +      throw new Error(`Hydration Error: ${e.message}`);
           }
         }
       
         asOf(date: Date): QuackGraph {
           // Return a shallow copy with new context
      -    const g = new QuackGraph(this.db['path'], { asOf: date });

      +    const g = new QuackGraph(this.db.path, { asOf: date });
           // Share the same DB connection and Native index (assuming topology is shared/latest)
           g.db = this.db;
           g.schema = this.schema;
      @@ -108,13 +111,15 @@
       
         // --- Write Operations (Write-Through) ---
       
      -  async addNode(id: string, labels: string[], props: Record<string, any>
      = {}) {

      +  // biome-ignore lint/suspicious/noExplicitAny: generic properties

      +  async addNode(id: string, labels: string[], props: Record<string, any>
      = {}) {
           // 1. Write to Disk (Source of Truth)
           await this.schema.writeNode(id, labels, props);
           // 2. Write to RAM (Cache)
           this.native.addNode(id);
         }
       
      -  async addEdge(source: string, target: string, type: string, props:
      Record<string, any> = {}) {

      +  // biome-ignore lint/suspicious/noExplicitAny: generic properties

      +  async addEdge(source: string, target: string, type: string, props:
      Record<string, any> = {}) {
           // 1. Write to Disk
           await this.schema.writeEdge(source, target, type, props);
           // 2. Write to RAM

    patchStrategy: standard-diff
  - type: write
    path: packages/quack-graph/src/schema.ts
    content: >+
      --- packages/quack-graph/src/schema.ts

      +++ packages/quack-graph/src/schema.ts

      @@ -1,4 +1,4 @@

      -import { DuckDBManager, type DbExecutor } from './db';

      +import type { DuckDBManager, DbExecutor } from './db';
       
       const NODES_TABLE = `
       CREATE TABLE IF NOT EXISTS nodes (
      @@ -32,7 +32,8 @@
           await this.db.execute(EDGES_TABLE);
         }
       
      -  async writeNode(id: string, labels: string[], properties:
      Record<string, any> = {}) {

      +  // biome-ignore lint/suspicious/noExplicitAny: generic properties

      +  async writeNode(id: string, labels: string[], properties:
      Record<string, any> = {}) {
           await this.db.transaction(async (tx: DbExecutor) => {
             // 1. Close existing record (SCD Type 2)
             await tx.execute(
      @@ -47,7 +48,8 @@
           });
         }
       
      -  async writeEdge(source: string, target: string, type: string,
      properties: Record<string, any> = {}) {

      +  // biome-ignore lint/suspicious/noExplicitAny: generic properties

      +  async writeEdge(source: string, target: string, type: string,
      properties: Record<string, any> = {}) {
           await this.db.transaction(async (tx: DbExecutor) => {
             // 1. Close existing edge
             await tx.execute(
      @@ -99,7 +101,7 @@
           try {
             // Note: DuckDB 0.9+ supports ADD COLUMN IF NOT EXISTS
             await this.db.execute(`ALTER TABLE nodes ADD COLUMN IF NOT EXISTS ${property} ${type}`);
      -    } catch (e) {

      +    } catch (_e) {
             // Fallback or ignore if column exists
           }
       

    patchStrategy: standard-diff
  - type: write
    path: test/utils/helpers.ts
    content: |+
      --- test/utils/helpers.ts
      +++ test/utils/helpers.ts
      @@ -21,7 +21,7 @@
         try {
           await unlink(path);
           // DuckDB creates a WAL file typically (.wal), try to remove it too just in case
           await unlink(`${path}.wal`).catch(() => {});
      -  } catch (e) {
      +  } catch (_e) {
           // Ignore errors if file doesn't exist
         }
       };

    patchStrategy: standard-diff
snapshot:
  crates/quack_core/src/topology.rs: |-
    use crate::interner::Interner;
    use bitvec::prelude::*;
    use std::collections::HashMap;
    use serde::{Serialize, Deserialize};
    use std::fs::File;
    use std::io::{BufReader, BufWriter};
    use arrow::record_batch::RecordBatch;
    use arrow::array::{AsArray, Array, ArrayRef, StringArray, LargeStringArray};
    use arrow::datatypes::DataType;

    /// The core Graph Index.
    /// Stores topology in RAM using integer IDs.
    #[derive(Default, Debug, Serialize, Deserialize)]
    pub struct GraphIndex {
        node_interner: Interner,
        
        // Mapping edge type strings (e.g. "KNOWS") to u8 for compact storage.
        // Limit: 256 edge types per graph in V1.
        edge_type_map: HashMap<String, u8>,
        edge_type_vec: Vec<String>,

        // Forward Graph: Source Node ID -> List of (Target Node ID, Edge Type ID)
        outgoing: Vec<Vec<(u32, u8)>>,
        
        // Reverse Graph: Target Node ID -> List of (Source Node ID, Edge Type ID)
        incoming: Vec<Vec<(u32, u8)>>,

        // Bitmask for soft-deleted nodes.
        // true = deleted (tombstone), false = active.
        tombstones: BitVec,
    }

    pub enum Direction {
        Outgoing,
        Incoming,
    }

    impl GraphIndex {
        pub fn new() -> Self {
            Self {
                node_interner: Interner::new(),
                edge_type_map: HashMap::new(),
                edge_type_vec: Vec::new(),
                outgoing: Vec::new(),
                incoming: Vec::new(),
                tombstones: BitVec::new(),
            }
        }

        /// Compacts internal vectors to minimize memory usage.
        /// Should be called after bulk hydration.
        pub fn compact(&mut self) {
            self.outgoing.shrink_to_fit();
            self.outgoing.iter_mut().for_each(|v| v.shrink_to_fit());
            self.incoming.shrink_to_fit();
            self.incoming.iter_mut().for_each(|v| v.shrink_to_fit());
            self.edge_type_vec.shrink_to_fit();
        }

        /// Resolves or creates an internal u32 ID for a node string.
        /// Resizes internal storage if necessary.
        pub fn get_or_create_node(&mut self, id: &str) -> u32 {
            let internal_id = self.node_interner.intern(id);
            let idx = internal_id as usize;

            // Ensure vectors are large enough to hold this node
            if idx >= self.outgoing.len() {
                let new_len = idx + 1;
                self.outgoing.resize_with(new_len, Vec::new);
                self.incoming.resize_with(new_len, Vec::new);
                // Resize tombstones, filling new slots with false (active)
                self.tombstones.resize(new_len, false);
            }
            internal_id
        }

        /// Marks a node as deleted (soft delete).
        /// Traversals will skip this node.
        pub fn remove_node(&mut self, id: &str) {
            if let Some(u_id) = self.node_interner.lookup_id(id) {
                let idx = u_id as usize;
                if idx < self.tombstones.len() {
                    self.tombstones.set(idx, true);
                }
            }
        }

        /// Returns the total number of edges in the graph.
        pub fn edge_count(&self) -> usize {
            self.outgoing.iter().map(|edges| edges.len()).sum()
        }

        /// Resolves or creates a u8 ID for an edge type string.
        /// Panics if more than 255 edge types are used (V1 constraint).
        pub fn get_or_create_type(&mut self, type_name: &str) -> u8 {
            if let Some(&id) = self.edge_type_map.get(type_name) {
                return id;
            }
            let id = self.edge_type_vec.len();
            if id > 255 {
                panic!("QuackGraph V1 Limit: Max 256 unique edge types supported.");
            }
            let id_u8 = id as u8;
            self.edge_type_vec.push(type_name.to_string());
            self.edge_type_map.insert(type_name.to_string(), id_u8);
            id_u8
        }

        /// Adds an edge to the graph. 
        /// Idempotent: Does not add duplicate edges if they already exist.
        pub fn add_edge(&mut self, source: &str, target: &str, edge_type: &str) {
            let u_src = self.get_or_create_node(source);
            let u_tgt = self.get_or_create_node(target);
            let u_type = self.get_or_create_type(edge_type);

            // Add to forward index (Idempotent)
            let out_vec = &mut self.outgoing[u_src as usize];
            if !out_vec.contains(&(u_tgt, u_type)) {
                out_vec.push((u_tgt, u_type));
            }
            
            // Add to reverse index (Idempotent)
            let in_vec = &mut self.incoming[u_tgt as usize];
            if !in_vec.contains(&(u_src, u_type)) {
                in_vec.push((u_src, u_type));
            }

            // Ensure nodes are not tombstoned if they are being re-added/linked
            if self.tombstones.get(u_src as usize).as_deref() == Some(&true) {
                self.tombstones.set(u_src as usize, false);
            }
            if self.tombstones.get(u_tgt as usize).as_deref() == Some(&true) {
                self.tombstones.set(u_tgt as usize, false);
            }
        }

        /// Removes a specific edge from the graph.
        /// Uses swap_remove for O(1) removal, order is not preserved.
        pub fn remove_edge(&mut self, source: &str, target: &str, edge_type: &str) {
            // We only proceed if all entities exist in our interner/maps
            if let (Some(u_src), Some(u_tgt), Some(u_type)) = (
                self.node_interner.lookup_id(source),
                self.node_interner.lookup_id(target),
                self.edge_type_map.get(edge_type).copied(),
            ) {
                // Remove from outgoing
                if let Some(edges) = self.outgoing.get_mut(u_src as usize) {
                    if let Some(pos) = edges.iter().position(|x| *x == (u_tgt, u_type)) {
                        edges.swap_remove(pos);
                    }
                }
                // Remove from incoming
                if let Some(edges) = self.incoming.get_mut(u_tgt as usize) {
                    if let Some(pos) = edges.iter().position(|x| *x == (u_src, u_type)) {
                        edges.swap_remove(pos);
                    }
                }
            }
        }

        /// Ingests an Apache Arrow RecordBatch directly.
        /// Expected Schema: Columns named "source", "target", "type" (case-insensitive or exact).
        pub fn add_arrow_batch(&mut self, batch: &RecordBatch) -> Result<(), String> {
            let schema = batch.schema();
            
            // Resolve column indices by name for robustness (Case-Insensitive)
            let find_col = |name: &str| -> Result<usize, String> {
                schema.fields().iter().position(|f| f.name().eq_ignore_ascii_case(name))
                    .ok_or_else(|| format!("Column '{}' not found in Arrow Batch. Available: {:?}", name, schema.fields().iter().map(|f| f.name()).collect::<Vec<_>>()))
            };

            let src_idx = find_col("source")?;
            let tgt_idx = find_col("target")?;
            let type_idx = find_col("type")?;

            let num_rows = batch.num_rows();
            if num_rows == 0 {
                return Ok(());
            }

            // Wrapper to handle different string array types (Utf8 vs LargeUtf8)
            enum StringArrayWrapper<'a> {
                Small(&'a StringArray),
                Large(&'a LargeStringArray),
            }

            impl<'a> StringArrayWrapper<'a> {
                fn value(&self, i: usize) -> &'a str {
                    match self {
                        Self::Small(arr) => arr.value(i),
                        Self::Large(arr) => arr.value(i),
                    }
                }
            }

            macro_rules! get_wrapper {
                ($col:expr, $name:expr) => {
                    match $col.data_type() {
                        DataType::Utf8 => StringArrayWrapper::Small($col.as_string::<i32>()),
                        DataType::LargeUtf8 => StringArrayWrapper::Large($col.as_string::<i64>()),
                        dt => return Err(format!("{} column: Unsupported type {:?}", $name, dt)),
                    }
                }
            }

            let src_wrapper = get_wrapper!(batch.column(src_idx), "Source");
            let tgt_wrapper = get_wrapper!(batch.column(tgt_idx), "Target");
            let type_wrapper = get_wrapper!(batch.column(type_idx), "Type");

            for i in 0..num_rows {
                self.add_edge(src_wrapper.value(i), tgt_wrapper.value(i), type_wrapper.value(i));
            }
            Ok(())
        }

        /// Generic traversal step (Bidirectional).
        /// Given a list of source node IDs (strings), find all neighbors connected by `edge_type`
        /// in the specified `direction`.
        pub fn traverse(&self, sources: &[String], edge_type: Option<&str>, direction: Direction) -> Vec<String> {
            let type_filter = edge_type.and_then(|t| self.edge_type_map.get(t).copied());
            
            let mut result_ids: Vec<u32> = Vec::with_capacity(sources.len() * 2);
            
            let adjacency = match direction {
                Direction::Outgoing => &self.outgoing,
                Direction::Incoming => &self.incoming,
            };

            for src_str in sources {
                // If source node doesn't exist in our index, skip it
                if let Some(src_id) = self.node_interner.lookup_id(src_str) {
                    // Check if node is deleted
                    if self.tombstones.get(src_id as usize).as_deref() == Some(&true) {
                        continue;
                    }

                    if let Some(edges) = adjacency.get(src_id as usize) {
                        for &(target, type_id) in edges {
                            // Apply edge type filter if present
                            if let Some(req_type) = type_filter {
                                if req_type != type_id {
                                    continue;
                                }
                            }
                            // Check if target is deleted
                            if self.tombstones.get(target as usize).as_deref() == Some(&true) {
                                continue;
                            }
                            result_ids.push(target);
                        }
                    }
                }
            }

            // Deduplicate results
            result_ids.sort_unstable();
            result_ids.dedup();

            // Convert back to strings
            result_ids
                .into_iter()
                .filter_map(|id| self.node_interner.lookup(id).map(|s| s.to_string()))
                .collect()
        }

        pub fn node_count(&self) -> usize {
            self.node_interner.len()
        }

        /// Serializes the entire graph topology to a binary file.
        pub fn save_to_file(&self, path: &str) -> Result<(), String> {
            let file = File::create(path).map_err(|e| e.to_string())?;
            let writer = BufWriter::new(file);
            bincode::serialize_into(writer, self).map_err(|e| e.to_string())?;
            Ok(())
        }

        /// Deserializes the graph topology from a binary file.
        pub fn load_from_file(path: &str) -> Result<Self, String> {
            let file = File::open(path).map_err(|e| e.to_string())?;
            let reader = BufReader::new(file);
            bincode::deserialize_from(reader).map_err(|e| e.to_string())
        }
    }
  packages/native/Cargo.toml: >-
    [package]

    edition = "2021"

    name = "quack_native"

    version = "0.0.1"


    [lib]

    crate-type = ["cdylib"]


    [dependencies]

    # Napi dependencies for bridging

    napi = { version = "2.12.2", default-features = false, features = ["napi4"]
    }

    napi-derive = "2.12.2"


    # Our Core Logic

    quack_core = { path = "../../crates/quack_core" }

    arrow = { version = "53.0.0" }


    [build-dependencies]

    napi-build = "2.0.1"


    [profile.release]

    lto = true
  biome.json: |-
    {
      "$schema": "https://biomejs.dev/schemas/1.9.4/schema.json",
      "vcs": {
        "enabled": true,
        "clientKind": "git",
        "useIgnoreFile": true
      },
      "files": {
        "ignoreUnknown": true,
        "includes": [
          "**",
          "!target/**",
          "!dist/**",
          "!node_modules/**",
          "!**/*.node",
          "!repomix.config.json"
        ]
      },
      "formatter": {
        "enabled": true,
        "indentStyle": "space",
        "indentWidth": 2,
        "lineWidth": 100
      },
      "linter": {
        "enabled": true,
        "rules": {
          "recommended": true
        }
      },
      "javascript": {
        "formatter": {
          "quoteStyle": "single",
          "trailingCommas": "es5"
        }
      }
    }
  packages/quack-graph/src/db.ts: >-
    import { Database } from 'duckdb-async';


    // Interface for operations that can be performed within a transaction or
    globally

    export interface DbExecutor {
      execute(sql: string, params?: any[]): Promise<void>;
      query(sql: string, params?: any[]): Promise<any[]>;
    }


    export class DuckDBManager implements DbExecutor {
      private db: Database | null = null;
      private path: string;

      constructor(path: string = ':memory:') {
        this.path = path;
      }

      async init() {
        if (!this.db) {
          this.db = await Database.create(this.path);
        }
      }

      getDb(): Database {
        if (!this.db) {
          throw new Error('Database not initialized. Call init() first.');
        }
        return this.db;
      }

      async execute(sql: string, params: any[] = []): Promise<void> {
        const db = this.getDb();
        await db.run(sql, params);
      }

      async query(sql: string, params: any[] = []): Promise<any[]> {
        const db = this.getDb();
        return await db.all(sql, params);
      }

      /**
       * Executes a callback within a transaction using a dedicated connection.
       * This guarantees that all operations inside the callback share the same ACID scope.
       */
      async transaction<T>(callback: (executor: DbExecutor) => Promise<T>): Promise<T> {
        const db = this.getDb();
        const conn = await db.connect();
        
        // Create a transaction-bound executor wrapper
        const txExecutor: DbExecutor = {
          execute: async (sql: string, params: any[] = []) => {
            await conn.run(sql, params);
          },
          query: async (sql: string, params: any[] = []) => {
            return await conn.all(sql, params);
          }
        };

        try {
          await conn.run('BEGIN TRANSACTION');
          const result = await callback(txExecutor);
          await conn.run('COMMIT');
          return result;
        } catch (e) {
          try {
            await conn.run('ROLLBACK');
          } catch (rollbackError) {
            console.error('Failed to rollback transaction:', rollbackError);
          }
          throw e;
        } finally {
          // Best effort close
          if (conn && typeof (conn as any).close === 'function') {
            (conn as any).close();
          }
        }
      }

      /**
       * Executes a query and returns the raw Apache Arrow IPC Buffer.
       * Used for high-speed hydration.
       */
      async queryArrow(sql: string, params: any[] = []): Promise<Uint8Array> {
        const db = this.getDb();
        
        return new Promise((resolve, reject) => {
          // Hack: Access underlying node-duckdb connection/database
          // duckdb-async instance holds 'db' property which is the native Database
          const rawDb = (db as any).db || db;

          if (!rawDb) return reject(new Error("Could not access underlying DuckDB Native instance."));

          // Helper to merge multiple Arrow batches if necessary
          const mergeBatches = (batches: Uint8Array[]) => {
            if (batches.length === 0) return new Uint8Array(0);
            if (batches.length === 1) return batches[0] ?? new Uint8Array(0);
            const totalLength = batches.reduce((acc, val) => acc + val.length, 0);
            const merged = new Uint8Array(totalLength);
            let offset = 0;
            for (const batch of batches) {
              merged.set(batch, offset);
              offset += batch.length;
            }
            return merged;
          };

          // Try Database.arrowIPCAll (available in newer node-duckdb)
          if (typeof rawDb.arrowIPCAll === 'function') {
            rawDb.arrowIPCAll(sql, params, (err: any, result: any) => {
              if (err) return reject(err);
              // Result is usually Array<Uint8Array> (batches)
              if (Array.isArray(result)) {
                resolve(mergeBatches(result));
              } else {
                resolve(result ?? new Uint8Array(0));
              }
            });
          } else {
             // Fallback: Create a raw connection
             try {
                const rawConn = rawDb.connect();
                
                // Handle case where rawDb is actually the connection itself (sometimes happens in certain pool configs)
                const target = typeof rawDb.arrowIPCAll === 'function' 
                  ? rawDb 
                  : (rawConn && typeof rawConn.arrowIPCAll === 'function' ? rawConn : null);

                if (target) {
                   target.arrowIPCAll(sql, params, (err: any, result: any) => {
                      if (err) return reject(err);
                      if (Array.isArray(result)) {
                        resolve(mergeBatches(result));
                      } else {
                        resolve(result ?? new Uint8Array(0));
                      }
                   });
                } else {
                   reject(new Error("Underlying DuckDB driver does not support arrowIPCAll"));
                }
             } catch(e) {
                reject(e);
             }
          }
        });
      }
    }
  packages/quack-graph/src/graph.ts: |-
    import { NativeGraph } from '@quackgraph/native';
    import { DuckDBManager } from './db';
    import { SchemaManager } from './schema';
    import { QueryBuilder } from './query';

    export class QuackGraph {
      db: DuckDBManager;
      schema: SchemaManager;
      native: NativeGraph;
      
      capabilities = {
        vss: false
      };

      // Context for the current instance (Time Travel)
      context: {
        asOf?: Date;
        topologySnapshot?: string;
      } = {};

      constructor(path: string = ':memory:', options: { asOf?: Date, topologySnapshot?: string } = {}) {
        this.db = new DuckDBManager(path);
        this.schema = new SchemaManager(this.db);
        this.native = new NativeGraph();
        this.context.asOf = options.asOf;
        this.context.topologySnapshot = options.topologySnapshot;
      }

      async init() {
        await this.db.init();
        
        // Load Extensions
        try {
          await this.db.execute("INSTALL vss; LOAD vss;");
          this.capabilities.vss = true;
        } catch (e) {
          console.warn("QuackGraph: Failed to load 'vss' extension. Vector search will be disabled.", e);
        }
        
        await this.schema.ensureSchema();
        
        // If we are in time-travel mode, we might skip hydration or hydrate a snapshot (Advanced).
        // For V1, we always hydrate "Current Active" topology.

        // Check for Topology Snapshot
        if (this.context.topologySnapshot) {
          try {
            // Try loading from disk
            this.native.loadSnapshot(this.context.topologySnapshot);
            // If successful, skip hydration
            return;
          } catch (e) {
            console.warn(`QuackGraph: Failed to load snapshot '${this.context.topologySnapshot}'. Falling back to full hydration.`, e);
          }
        }

        try {
          await this.hydrate();
        } catch (e) {
          console.error("Failed to hydrate graph topology from disk:", e);
          // We don't throw here to allow partial functionality (metadata queries) if needed,
          // but usually this is fatal for graph operations.
          throw e;
        }
      }

      /**
       * Hydrates the in-memory Rust graph from the persistent DuckDB storage.
       * This is critical for the "Split-Brain" architecture.
       */
      async hydrate() {
        // Zero-Copy Arrow IPC
        // 'valid_to IS NULL' ensures we only load currently active edges.
        try {
          const ipcBuffer = await this.db.queryArrow(
            "SELECT source, target, type FROM edges WHERE valid_to IS NULL"
          );
        
          if (ipcBuffer && ipcBuffer.length > 0) {
             // Napi-rs expects a Buffer or equivalent
             // Buffer.from is zero-copy in Node for Uint8Array usually, or cheap copy
             // We cast to any to satisfy the generated TS definitions which might expect Buffer
             const bufferForNapi = Buffer.isBuffer(ipcBuffer) 
                ? ipcBuffer 
                : Buffer.from(ipcBuffer);
                
             this.native.loadArrowIpc(bufferForNapi as any);

             // Reclaim memory after burst hydration
             this.native.compact();
          }
        } catch (e: any) {
          throw new Error(`Hydration Error: ${e.message}`);
        }
      }

      asOf(date: Date): QuackGraph {
        // Return a shallow copy with new context
        const g = new QuackGraph(this.db['path'], { asOf: date });
        // Share the same DB connection and Native index (assuming topology is shared/latest)
        g.db = this.db;
        g.schema = this.schema;
        g.native = this.native;
        g.capabilities = { ...this.capabilities };
        return g;
      }

      // --- Write Operations (Write-Through) ---

      async addNode(id: string, labels: string[], props: Record<string, any> = {}) {
        // 1. Write to Disk (Source of Truth)
        await this.schema.writeNode(id, labels, props);
        // 2. Write to RAM (Cache)
        this.native.addNode(id);
      }

      async addEdge(source: string, target: string, type: string, props: Record<string, any> = {}) {
        // 1. Write to Disk
        await this.schema.writeEdge(source, target, type, props);
        // 2. Write to RAM
        this.native.addEdge(source, target, type);
      }

      async deleteNode(id: string) {
        // 1. Write to Disk (Soft Delete)
        await this.schema.deleteNode(id);
        // 2. Write to RAM (Tombstone)
        this.native.removeNode(id);
      }

      async deleteEdge(source: string, target: string, type: string) {
        // 1. Write to Disk (Soft Delete)
        await this.schema.deleteEdge(source, target, type);
        // 2. Write to RAM (Remove)
        this.native.removeEdge(source, target, type);
      }

      // --- Optimization & Maintenance ---

      get optimize() {
        return {
          promoteProperty: async (label: string, property: string, type: string) => {
            await this.schema.promoteNodeProperty(label, property, type);
          },
          saveTopologySnapshot: (path: string) => {
            this.native.saveSnapshot(path);
          }
        };
      }

      // --- Read Operations ---

      match(labels: string[]): QueryBuilder {
        return new QueryBuilder(this, labels);
      }
    }
  packages/quack-graph/src/schema.ts: |-
    import { DuckDBManager, type DbExecutor } from './db';

    const NODES_TABLE = `
    CREATE TABLE IF NOT EXISTS nodes (
        row_id UBIGINT PRIMARY KEY, -- Simple auto-increment equivalent logic handled by sequence
        id TEXT NOT NULL,
        labels TEXT[],
        properties JSON,
        embedding FLOAT[], -- Vector embedding
        valid_from TIMESTAMP DEFAULT current_timestamp,
        valid_to TIMESTAMP DEFAULT NULL
    );
    CREATE SEQUENCE IF NOT EXISTS seq_node_id;
    `;

    const EDGES_TABLE = `
    CREATE TABLE IF NOT EXISTS edges (
        source TEXT NOT NULL,
        target TEXT NOT NULL,
        type TEXT NOT NULL,
        properties JSON,
        valid_from TIMESTAMP DEFAULT current_timestamp,
        valid_to TIMESTAMP DEFAULT NULL
    );
    `;

    export class SchemaManager {
      constructor(private db: DuckDBManager) {}

      async ensureSchema() {
        await this.db.execute(NODES_TABLE);
        await this.db.execute(EDGES_TABLE);
      }

      async writeNode(id: string, labels: string[], properties: Record<string, any> = {}) {
        await this.db.transaction(async (tx: DbExecutor) => {
          // 1. Close existing record (SCD Type 2)
          await tx.execute(
            `UPDATE nodes SET valid_to = current_timestamp WHERE id = ? AND valid_to IS NULL`,
            [id]
          );
          // 2. Insert new version
          await tx.execute(`
            INSERT INTO nodes (row_id, id, labels, properties, valid_from, valid_to) 
            VALUES (nextval('seq_node_id'), ?, ?, ?::JSON, current_timestamp, NULL)
          `, [id, labels, JSON.stringify(properties)]);
        });
      }

      async writeEdge(source: string, target: string, type: string, properties: Record<string, any> = {}) {
        await this.db.transaction(async (tx: DbExecutor) => {
          // 1. Close existing edge
          await tx.execute(
            `UPDATE edges SET valid_to = current_timestamp WHERE source = ? AND target = ? AND type = ? AND valid_to IS NULL`,
            [source, target, type]
          );
          // 2. Insert new version
          await tx.execute(`
            INSERT INTO edges (source, target, type, properties, valid_from, valid_to) 
            VALUES (?, ?, ?, ?::JSON, current_timestamp, NULL)
          `, [source, target, type, JSON.stringify(properties)]);
        });
      }

      async deleteNode(id: string) {
        // Soft Delete: Close the validity period
        await this.db.transaction(async (tx: DbExecutor) => {
          await tx.execute(
            `UPDATE nodes SET valid_to = current_timestamp WHERE id = ? AND valid_to IS NULL`,
            [id]
          );
        });
      }

      async deleteEdge(source: string, target: string, type: string) {
        // Soft Delete: Close the validity period
        await this.db.transaction(async (tx: DbExecutor) => {
          await tx.execute(
            `UPDATE edges SET valid_to = current_timestamp WHERE source = ? AND target = ? AND type = ? AND valid_to IS NULL`,
            [source, target, type]
          );
        });
      }

      /**
       * Promotes a JSON property to a native column for faster filtering.
       * This creates a column on the `nodes` table and backfills it from the `properties` JSON blob.
       * 
       * @param label The node label to target (e.g., 'User'). Only nodes with this label will be updated.
       * @param property The property key to promote (e.g., 'age').
       * @param type The DuckDB SQL type (e.g., 'INTEGER', 'VARCHAR').
       */
      async promoteNodeProperty(label: string, property: string, type: string) {
        // Sanitize inputs to prevent basic SQL injection (rudimentary check)
        if (!/^[a-zA-Z0-9_]+$/.test(property)) throw new Error(`Invalid property name: '${property}'. Must be alphanumeric + underscore.`);
        // Type check is looser to allow various SQL types, but strictly alphanumeric + spaces/parens usually safe enough for now
        if (!/^[a-zA-Z0-9_() ]+$/.test(type)) throw new Error(`Invalid SQL type: '${type}'.`);
        // Sanitize label just in case, though it is used as a parameter usually, here we might need dynamic check if we were using it in table names, but we use it in list_contains param.
        
        // 1. Add Column (Idempotent)
        try {
          // Note: DuckDB 0.9+ supports ADD COLUMN IF NOT EXISTS
          await this.db.execute(`ALTER TABLE nodes ADD COLUMN IF NOT EXISTS ${property} ${type}`);
        } catch (e) {
          // Fallback or ignore if column exists
        }

        // 2. Backfill Data
        // We use list_contains to only update relevant nodes
        const sql = `
          UPDATE nodes 
          SET ${property} = CAST(json_extract(properties, '$.${property}') AS ${type})
          WHERE list_contains(labels, ?)
        `;
        await this.db.execute(sql, [label]);
      }
    }
  test/utils/helpers.ts: >-
    import { unlink } from 'node:fs/promises';

    import { join } from 'node:path';

    import { tmpdir } from 'node:os';

    import { QuackGraph } from '../../packages/quack-graph/src/index';


    export const getTempPath = (prefix = 'quack-test') => {
      const uuid = crypto.randomUUID();
      return join(tmpdir(), `${prefix}-${uuid}.duckdb`);
    };


    export const createGraph = async (mode: 'memory' | 'disk' = 'memory') => {
      const path = mode === 'memory' ? ':memory:' : getTempPath();
      const graph = new QuackGraph(path);
      await graph.init();
      return { graph, path };
    };


    export const cleanupGraph = async (path: string) => {
      if (path === ':memory:') return;
      try {
        await unlink(path);
        // DuckDB creates a WAL file typically (.wal), try to remove it too just in case
        await unlink(`${path}.wal`).catch(() => {});
      } catch (e) {
        // Ignore errors if file doesn't exist
      }
    };


    /**
     * Wait for a short duration. Useful if we need to ensure timestamps differ slightly
     * (though QuackGraph uses microsecond precision usually, node might be ms).
     */
    export const sleep = (ms: number) => new Promise((resolve) =>
    setTimeout(resolve, ms));
approved: true
linesAdded: 33
linesRemoved: 16
linesDifference: 49
gitCommittedAt: null
status: committed
id: 20
